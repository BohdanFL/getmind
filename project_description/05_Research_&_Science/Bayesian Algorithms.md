Я дізнався про це з книги Як ми вчимось Станіслава Деана.

Потім поліз шукати про це інфу в гпт, дізнався що це таке. Далі заліз в гугл ші студіо. 
Побачив які є можливості застосування. Поліз читати про bkt (bayesan knowledge tracing) та наткнувся на ITS (intelligent tutoring system).
А далі там пішло поїхало.

Отож про баєсівські алгоритми. Це ймовірні алгоритми які базується на такій формулі

$$
P(гіпотеза | дані) = P(дані|гіпотеза)*\frac{P(гіпотеза)}{P(дані)} 
$$

ЇЇ суть така, що ми оновлюємо ймовірність гіпотези після отримання нових знань.

Тобто P(дані|гіпотеза) - це ймовірність результату за, точніше очікуваність даних за істинності гіпотези
P(гіпотеза) - початкове припущення
P(дані) -
І результатом буде ймовірність за наявних даних. Оновлена.

Далі я дізнався вже про сам BKT що це якраз стала практика для визначення рівня опанування навичок учня.

Потмі ще одне застосування це динамічне управління складністю. Тобто тут ми коригуємо desirable difficulties. Якщо учень добре відпоівдає, то ймовірність, що завдання легкі зростає і при певному порозі P > 0.9 ми починаємо змінювати поведінку системи. Тобто прибирати підказки, виходити на новий рівень Блума і т.д. Якщо навпаки P < 0.3 То змінюємо підхід і додаємо додаткові аналогії, спрощуємо завданян і опускаємось на нижчий рівень блума.

далі ми можемо за допогою цього алгоритму працювати з ілюзіями знання.
Тобто ми оцінюємо наскільки впевнений у своїй відповідь учень, тоді порівнюємо з реальністю і кажемо чи це умовно ілюзія знання чи недостатня впевненність. Тоді маємо відреагувати відповідно, понизити показники майстерності і додати якісь додаткові завдання.
Плюс може бути додаткова графа "впевненність у своїх знаннях". Яку можна відслідковувати і робити певну кореляцію реальних знань те впевненності, що дасть учню додатку інфу.

Далі менш цікавий підхід це bayesian spaced repetition.
Це просто підхід тривалого навчання, там вже свої алгоритми і з цим я розберусь ще.


#confidence-based_learning #desirable_difficulties #knowledge_map #knowledge_tracing
